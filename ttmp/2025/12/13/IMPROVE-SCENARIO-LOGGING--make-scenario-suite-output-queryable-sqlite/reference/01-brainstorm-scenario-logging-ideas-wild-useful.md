---
Title: 'Brainstorm: scenario logging ideas (wild + useful)'
Ticket: IMPROVE-SCENARIO-LOGGING
Status: active
Topics:
    - testing
    - tooling
    - diagnostics
DocType: reference
Intent: long-term
Owners: []
RelatedFiles:
    - Path: test-scenarios/testing-doc-manager/run-all.sh
      Note: Harness entrypoint we’ll instrument first
    - Path: test-scenarios/testing-doc-manager/README.md
      Note: Scenario suite docs + execution conventions
    - Path: ttmp/2025/12/13/IMPROVE-SCENARIO-LOGGING--make-scenario-suite-output-queryable-sqlite/design-doc/01-scenario-suite-structured-logging-sqlite.md
      Note: Scenario-specific baseline design (schema + bash wrapper)
    - Path: ttmp/2025/12/13/IMPROVE-SCENARIO-LOGGING--make-scenario-suite-output-queryable-sqlite/design-doc/02-generic-sqlite-scenario-logger-go-tool.md
      Note: Generalized reusable Go tool design that can implement many ideas below
    - Path: internal/workspace/sqlite_schema.go
      Note: Existing sqlite patterns in docmgr (pragmas, quoting helper)
ExternalSources: []
Summary: "Idea-bank for turning scenario runs into a queryable sqlite flight recorder: schema expansions, capture tactics, reports, diffing, CI, and weird-but-useful extras."
LastUpdated: 2025-12-13T16:43:14.064674334-05:00
---

# Brainstorm: scenario logging ideas (wild + useful)

## Goal

Generate a deliberately wide idea-space for **scenario-style logging to sqlite** (runs/steps/commands + artifacts), with enough structure that we can:

- pick a sane MVP (Phase 1)
- keep a backlog of spicy upgrades (Phases 2+)
- reuse the same tool/approach outside `docmgr` scenarios

## Context

We already have a concrete scenario-specific design in design-doc #1 (sqlite schema + bash wrapper) and a generalized reusable Go-tool design in design-doc #2.

This document is intentionally **not** a strict spec. It’s a desert-retreat idea dump: some items are “ship it tomorrow”, others are “ship it if we ever want to feel like wizards”.

Constraints that keep showing up in practice:

- Scenario suites produce **big mixed output** (harness markers + command output + diagnostics).
- We want queries that answer questions *fast* without regex spelunking.
- We want to preserve “raw artifacts” (stdout/stderr) but also have **structured facts** (what failed, how long, why).
- We want the same approach to work in:
  - bash harnesses
  - Go harnesses
  - CI (artifact upload + deterministic summaries)

## Quick Reference

### North Star Metaphor: the Flight Recorder

Don’t think “log file”. Think **flight recorder**:

- Every run is a “flight”.
- Steps are “phases”.
- Commands are “maneuvers”.
- Artifacts are “black box tapes”.
- Diagnostics are the “investigator notes”.

If we can answer these in seconds, we win:

- “What failed, where, and what’s the exact stderr?”
- “What changed compared to last run?”
- “What got slower, and by how much?”
- “Which warnings are new vs known?”
- “Can I hand someone one artifact and have them reproduce the failure?”

### Idea Menu (pick-your-own-adventure)

Below are ideas grouped by theme. Each item has a rough “vibe”:

- **MVP**: low-risk, high-value
- **Power**: bigger lift, high payoff
- **Wizardry**: questionable, delightful, sometimes extremely effective

---

### 1) Schema expansions that stay sane

- **MVP: `tags` / `kv` table**
  - Add arbitrary `key/value` for `run_id`, `step_id`, `command_id`.
  - Use cases: git SHA, host, suite name/version, “expected failure”, scenario parameters.

- **MVP: normalize time**
  - Store timestamps in RFC3339Nano plus `duration_ms` (or `duration_ns`).
  - Add `started_at_unix_ms` numeric for faster comparisons and sorting.

- **MVP: store argv structurally**
  - `command_argv0`, `command_args_json` (JSON array), `cwd`, maybe `env_allowlist_json`.
  - Avoid “command TEXT” becoming an unparseable string blob.

- **Power: `artifacts` table**
  - One table for all files generated by the run (stdout/stderr are just two artifact kinds).
  - Fields: `kind` (stdout/stderr/screenshot/archive/trace), `path`, `size_bytes`, `sha256`, `mime`, `preview_text` (optional).

- **Power: `expectations` tables**
  - Encode “this step is allowed to warn” or “this step is expected to fail once”.
  - Tables:
    - `expected_failures(step_name, pattern, rationale, expires_at)`
    - `expected_warnings(step_name, pattern, rationale, expires_at)`
  - Lets CI fail on *new* warnings while tolerating known ones.

- **Wizardry: single `events` table**
  - Everything becomes events:
    - `event_id, run_id, ts, kind, scope_id, payload_json`
  - Pros: extremely flexible.
  - Cons: queries get harder unless you build views/materialized tables.

- **Wizardry: sqlite FTS index for log search**
  - Create a `log_lines` table (one row per line) and FTS5 index it.
  - Query “warnings across all runs” in milliseconds.
  - Trade-off: more ingestion work, bigger DB; but feels like magic.

---

### 2) Capture strategies (how to bottle lightning)

- **MVP: step-level capture always**
  - Capture stdout/stderr per step.
  - The DB stores “where to look” + timings + exit code.

- **MVP: optional tee to console**
  - Keep local dev UX by streaming while also saving files.
  - Allow `--tee`, `--tee-stdout`, `--tee-stderr`.

- **Power: command-level capture on demand**
  - For noisy steps with many commands, capture each command separately.
  - Tooling can still also keep step-level log as the “big tape”.

- **Power: process group + signal hygiene**
  - On CTRL-C or timeout, kill the **entire process tree**.
  - Record in DB whether termination was signal-based (`killed_by_signal`, `signal_name`).

- **Power: resource metrics**
  - Capture:
    - `max_rss_kb`
    - `user_cpu_ms`, `sys_cpu_ms`
    - `io_read_bytes`, `io_write_bytes` (if accessible)
  - Enables “this step got slower because it’s paging”.

- **Wizardry: deterministic replay**
  - Record:
    - environment allowlist
    - exact argv
    - working directory
    - input files (hashes)
  - Provide `replay` command that reconstructs the step locally.

- **Wizardry: “log compression with indexing”**
  - Store `.logs/*.txt.zst` plus a small index for random access.
  - Lets you keep years of runs without exploding disk usage.

---

### 3) Diagnostics extraction (turn chaos into facts)

- **MVP: doctor JSON ingestion**
  - If a command supports `--diagnostics-json`, store it verbatim and/or parse into `diagnostics` rows.
  - For docmgr specifically: run `docmgr doctor --diagnostics-json ...` and ingest.

- **MVP: error fingerprinting**
  - For each non-zero exit, generate a stable “fingerprint”:
    - normalize paths, numbers, UUIDs
    - hash the normalized first 20 stderr lines
  - Enables grouping failures across runs.

- **Power: taxonomy**
  - `category`: `unknown_flag`, `missing_file`, `frontmatter_invalid`, `sqlite_locked`, `network`, etc.
  - Start with heuristics; evolve to rule-based classification.

- **Power: “expected vs unexpected” classification**
  - Compare discovered diagnostics to allowlists/expectations.
  - CI can fail only on unexpected categories.

- **Wizardry: causal links**
  - Detect:
    - step A produced file X
    - step B failed because X was missing
  - Store as `edges(from_step_id, to_step_id, kind, evidence)`.
  - Generates a “why graph” of failure propagation.

---

### 4) Reporting UX (how humans touch the data)

- **MVP: report commands**
  - `summary`: exit code, duration, failed steps
  - `timings`: slowest steps/commands
  - `failures`: show failing stderr paths
  - `warnings`: quick scan based on regex in artifacts

- **MVP: stable output modes**
  - Human default + machine mode:
    - `--output json`
    - (optional) `--with-glaze-output` style, if we want docmgr-like scriptability

- **Power: HTML report**
  - One file: `report.html` with:
    - run metadata
    - expandable steps
    - inline stderr previews
    - timing bars
  - This is a “shareable artifact” for PR reviews.

- **Power: web UI (local)**
  - `scenariolog web --db ...` that serves a tiny dashboard.
  - If implemented in Go: use **templ + htmx + bootstrap** for low-friction UI.

- **Wizardry: interactive TUI**
  - Browse runs/steps like `git log`, open logs with `less`, run queries.

- **Wizardry: “postmortem generator”**
  - Command that outputs a markdown incident-style summary:
    - “What failed”
    - “First error”
    - “Likely root cause”
    - “Diff vs last green run”
  - Works great with LLMs when paired with the DB + a few selected artifacts.

---

### 5) Diffing + regression detection (time as a dimension)

- **MVP: compare two runs**
  - Diff by `step_num`/`step_name`.
  - Show duration deltas + new failures.

- **Power: baseline + thresholds**
  - Store “golden run” per suite.
  - Fail CI if:
    - any new unexpected diagnostic appears
    - duration delta exceeds threshold (absolute or percent)

- **Power: anomaly detection**
  - Track durations over time.
  - Alert on statistical outliers (z-score, MAD).

- **Wizardry: automatic bisect**
  - When CI sees regression, automatically bisect commits using the scenario suite and log each run into the DB.
  - The DB becomes the bisect timeline.

---

### 6) Reproducibility + provenance (the run should explain itself)

- **MVP: provenance snapshot**
  - Store:
    - suite name/version
    - binary path + `--version` output (if available)
    - user, hostname
    - OS/arch
    - git SHA + dirty flag (if inside git repo)

- **Power: environment allowlist snapshot**
  - Capture only whitelisted env vars to avoid secrets.
  - Example allowlist: `PATH`, `HOME`, `DOCMGR_*`, `CI`, `GITHUB_*` (careful).

- **Power: “bundle” command**
  - `bundle --run-id ... --out run-bundle.tgz`
  - Include:
    - sqlite db (or filtered subset)
    - logs
    - key referenced files (optional)
  - Makes bug reports portable.

- **Wizardry: “narrative timeline”**
  - Convert the run into an annotated timeline (“step 03 waited 12s, likely IO-bound”).

---

### 7) Retention + hygiene (don’t create an eternal landfill)

- **MVP: user-managed cleanup**
  - Document “delete `.logs/` and DB when you’re done”.

- **Power: prune command**
  - Keep last N runs, or last N days, or keep only failing runs + last 2 green runs.
  - Vacuum + analyze occasionally.

- **Wizardry: tiered storage**
  - Keep summaries forever; keep full logs only for:
    - failing runs
    - runs referenced by a ticket/PR

---

### 8) Wild-but-useful extras (the fun part)

- **Run as a “spell”**
  - Define steps in a small YAML spec; tool executes them.
  - This slowly replaces ad-hoc bash over time.

- **Artifacts beyond logs**
  - Screenshots (if browser tests)
  - Generated docs
  - SQLite exports
  - Diff outputs

- **“Noise budget”**
  - Define a maximum number of warnings allowed per category.
  - Fail if exceeded, even if they’re “expected”.

- **Self-annotating steps**
  - Steps can emit a one-line JSON “annotation” to stdout; the tool parses and stores it as structured metadata:
    - `{"kind":"note","message":"Using docmgr binary from /tmp/docmgr-local"}`

- **Error haikus**
  - When the run fails, generate a short “failure signature” line that’s stable and copy/pasteable.
  - (Surprisingly useful for searching Slack/issues.)

---

### Query Cookbook (copy/paste)

#### Failed steps across all runs

```sql
SELECT run_id, step_num, step_name, exit_code, stderr_path
FROM steps
WHERE exit_code != 0
ORDER BY started_at DESC, step_num;
```

#### Slowest steps in the most recent run

```sql
WITH latest AS (
  SELECT run_id FROM scenario_runs ORDER BY started_at DESC LIMIT 1
)
SELECT step_num, step_name, duration_ms
FROM steps
WHERE run_id = (SELECT run_id FROM latest)
ORDER BY duration_ms DESC
LIMIT 10;
```

#### Find runs with “unknown flag” (heuristic search in stderr artifacts)

```sql
SELECT run_id, step_num, step_name, stderr_path
FROM steps
WHERE stderr_path IS NOT NULL
ORDER BY started_at DESC;
```

(Then grep the artifacts; if we later add FTS, this becomes a direct SQL query.)

### Prompt Pack (copy/paste into an LLM session)

Use this when you have a failing scenario run and want a fast diagnosis:

```text
You are diagnosing a scenario run. You will be given:
- run metadata (run_id, suite, started_at, exit_code)
- a list of steps (step_num, step_name, duration_ms, exit_code, stdout_path, stderr_path)
- stderr excerpts from failing steps

Tasks:
1) Identify the earliest failure (first non-zero step_num) and summarize it in one sentence.
2) Extract the likely root cause and propose the minimal fix.
3) List follow-up questions if the root cause is ambiguous.
4) Suggest 1-2 additional queries (SQL or grep) that would increase confidence.
Prefer precision over speculation.
```

## Usage Examples

### Use this idea bank to choose a Phase 1 MVP

Pick 3–5 MVP bullets above, then implement them in the current suite:

- step-level capture + exit codes + duration
- run metadata + provenance
- summary reporter

### Use this idea bank to define “Phase 2 spice”

When you hit a real pain point (e.g., “step 12 runs 10 commands and I can’t tell which failed”), pick the matching “Power” item and add it.

## Related

- Design-doc #1: `ttmp/2025/12/13/IMPROVE-SCENARIO-LOGGING--make-scenario-suite-output-queryable-sqlite/design-doc/01-scenario-suite-structured-logging-sqlite.md`
- Design-doc #2: `ttmp/2025/12/13/IMPROVE-SCENARIO-LOGGING--make-scenario-suite-output-queryable-sqlite/design-doc/02-generic-sqlite-scenario-logger-go-tool.md`
- Scenario suite: `test-scenarios/testing-doc-manager/`
